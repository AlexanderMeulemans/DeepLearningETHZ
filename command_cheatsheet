python -m pip install --user tqdm 
rsync -Pav /home/username/directory/ abally@login.leonhard.ethz.ch:/cluster/home/username/directory
acces hard drive through terminal: 
lsblk
go to path that appears (cd /run/media/ameulema/Elements)

submit job:
bsub -n 1 -W 4:00 -R "rusage[mem=2048, ngpus_excl_p=1]" python tftest1.py
bjobs
1023622 

load environment: module load python_gpu/3.6.4
https://github.com/AlexanderMeulemans/DeepLearningETHZ.git

copy training folder to scratch: cp -r train_reduced $SCRATCH/DeepLearningETHZ

Run ResNet: bsub -n 1 -R "rusage[mem=2048, ngpus_excl_p=1]" python ResNet18.py --data_augmentation standard --resultfile run_resnet.csv --datafolder train_reduced --infofile final_train_info.csv

DAGAN arguments: --batch_size 32 --generator_inner_layers 3 --discriminator_inner_layers 5 --num_generations 64 --experiment_title omniglot_dagan_experiment_default --num_of_gpus 1 --z_dim 100 --dropout_rate_value 0.5
1030753 

python train_omniglot_dagan.py --batch_size 8 --generator_inner_layers 2 --discriminator_inner_layers 4 --num_generations 64 --experiment_title omniglot_dagan_experiment_default --num_of_gpus 1 --z_dim 100 --dropout_rate_value 0.5




load environment: module load python_gpu/3.6.4
bsub -n 20 -W 120:00 -R "rusage[mem=4500, ngpus_excl_p=1]" python train_painting_styles.py --batch_size 8 --generator_inner_layers 2 --discriminator_inner_layers 4 --num_generations 64 --experiment_title paintingstyles2 --num_of_gpus 1 --z_dim 100 --dropout_rate_value 0.5 --infofile ../data_info_files/final_train_info.csv --data_dir ../train_reduced

run on omniglot: 
bsub -n 20 -W 4:00 -R "rusage[mem=4500, ngpus_excl_p=1]" python train_omniglot_dagan.py --batch_size 32 --generator_inner_layers 3 --discriminator_inner_layers 5 --num_generations 64 --experiment_title omniglot --num_of_gpus 1 --z_dim 100 --dropout_rate_value 0.5